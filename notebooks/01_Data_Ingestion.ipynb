{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  1) Install Hadoop"
      ],
      "metadata": {
        "id": "EKGJP-GW8ZsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-11-jdk-headless\n",
        "\n",
        "# Download and extract Hadoop 3.3.6\n",
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar -xzf hadoop-3.3.6.tar.gz\n",
        "!mv hadoop-3.3.6 /usr/local/hadoop\n",
        "\n",
        "# Set environment variables for this session\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"PATH\"] += \":/usr/local/hadoop/bin:/usr/local/hadoop/sbin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W0uieH_8eve",
        "outputId": "8d5012e9-8fb6-4213-e397-78b4d9af8555"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jdk-headless is already the newest version (11.0.28+6-1ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "mv: cannot move 'hadoop-3.3.6' to '/usr/local/hadoop/hadoop-3.3.6': Directory not empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Configure HDFS (single-node) & format"
      ],
      "metadata": {
        "id": "iNh8dVyD8jWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Format NameNode and start HDFS (NameNode + DataNode) ===\n",
        "import os, pathlib, subprocess\n",
        "\n",
        "HADOOP_HOME = \"/usr/local/hadoop\"\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# Ensure env vars are available\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"HADOOP_HOME\"] = HADOOP_HOME\n",
        "os.environ[\"PATH\"] = f\"{HADOOP_HOME}/bin:{HADOOP_HOME}/sbin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "# Create HDFS directories for NameNode and DataNode\n",
        "nn_dir = \"/content/hdfs/namenode\"\n",
        "dn_dir = \"/content/hdfs/datanode\"\n",
        "pathlib.Path(nn_dir).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(dn_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Append JAVA_HOME to hadoop-env.sh\n",
        "with open(f\"{HADOOP_HOME}/etc/hadoop/hadoop-env.sh\", \"a\") as f:\n",
        "    f.write(f\"\\nexport JAVA_HOME={JAVA_HOME}\\n\")\n",
        "\n",
        "# Write minimal configs\n",
        "core_site = \"\"\"\\\n",
        "<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "  <property>\n",
        "    <name>fs.defaultFS</name>\n",
        "    <value>hdfs://localhost:9000</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>hadoop.tmp.dir</name>\n",
        "    <value>/content/hdfs/tmp</value>\n",
        "  </property>\n",
        "</configuration>\n",
        "\"\"\"\n",
        "with open(f\"{HADOOP_HOME}/etc/hadoop/core-site.xml\", \"w\") as f:\n",
        "    f.write(core_site)\n",
        "\n",
        "hdfs_site = f\"\"\"\\\n",
        "<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "  <property>\n",
        "    <name>dfs.replication</name>\n",
        "    <value>1</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>dfs.namenode.name.dir</name>\n",
        "    <value>file:{nn_dir}</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>dfs.datanode.data.dir</name>\n",
        "    <value>file:{dn_dir}</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>dfs.namenode.http-address</name>\n",
        "    <value>0.0.0.0:9870</value>\n",
        "  </property>\n",
        "</configuration>\n",
        "\"\"\"\n",
        "with open(f\"{HADOOP_HOME}/etc/hadoop/hdfs-site.xml\", \"w\") as f:\n",
        "    f.write(hdfs_site)\n",
        "\n",
        "# Format the NameNode (safe to rerun only on fresh setup)\n",
        "print(\"Formatting NameNode…\")\n",
        "!$HADOOP_HOME/bin/hdfs namenode -format -nonInteractive\n",
        "\n",
        "# Start NameNode and DataNode\n",
        "print(\"Starting NameNode and DataNode…\")\n",
        "!$HADOOP_HOME/bin/hdfs --daemon start namenode\n",
        "!$HADOOP_HOME/bin/hdfs --daemon start datanode\n",
        "\n",
        "# Show running Java processes\n",
        "print(\"\\nJava processes (via jps):\")\n",
        "!jps || true\n",
        "\n",
        "# Create base HDFS dirs\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root || true\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir -p /data || true\n",
        "\n",
        "print(\"\\nCheck HDFS root directory:\")\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /"
      ],
      "metadata": {
        "id": "oQEXGcUs8n1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a81fca56-67ba-4598-f97c-9028d63f31e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting NameNode…\n",
            "WARNING: /usr/local/hadoop/logs does not exist. Creating.\n",
            "2025-09-19 12:52:39,521 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 9c5f6c881f56/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format, -nonInteractive]\n",
            "STARTUP_MSG:   version = 3.3.6\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z\n",
            "STARTUP_MSG:   java = 11.0.28\n",
            "************************************************************/\n",
            "2025-09-19 12:52:39,569 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2025-09-19 12:52:39,809 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]\n",
            "2025-09-19 12:52:40,963 INFO namenode.NameNode: Formatting using clusterid: CID-e5592325-0cc0-450c-8d93-3b4f5c07b242\n",
            "2025-09-19 12:52:41,050 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2025-09-19 12:52:41,123 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2025-09-19 12:52:41,126 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2025-09-19 12:52:41,126 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2025-09-19 12:52:41,192 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2025-09-19 12:52:41,192 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2025-09-19 12:52:41,192 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2025-09-19 12:52:41,192 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2025-09-19 12:52:41,192 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2025-09-19 12:52:41,277 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2025-09-19 12:52:41,528 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2025-09-19 12:52:41,528 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2025-09-19 12:52:41,547 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2025-09-19 12:52:41,549 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Sep 19 12:52:41\n",
            "2025-09-19 12:52:41,552 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2025-09-19 12:52:41,552 INFO util.GSet: VM type       = 64-bit\n",
            "2025-09-19 12:52:41,554 INFO util.GSet: 2.0% max memory 3.2 GB = 64.9 MB\n",
            "2025-09-19 12:52:41,554 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2025-09-19 12:52:41,584 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2025-09-19 12:52:41,584 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2025-09-19 12:52:41,591 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2025-09-19 12:52:41,591 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2025-09-19 12:52:41,591 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2025-09-19 12:52:41,592 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2025-09-19 12:52:41,592 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2025-09-19 12:52:41,592 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2025-09-19 12:52:41,592 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2025-09-19 12:52:41,593 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2025-09-19 12:52:41,593 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2025-09-19 12:52:41,593 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2025-09-19 12:52:41,648 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2025-09-19 12:52:41,648 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2025-09-19 12:52:41,648 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2025-09-19 12:52:41,648 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2025-09-19 12:52:41,663 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2025-09-19 12:52:41,663 INFO util.GSet: VM type       = 64-bit\n",
            "2025-09-19 12:52:41,663 INFO util.GSet: 1.0% max memory 3.2 GB = 32.5 MB\n",
            "2025-09-19 12:52:41,663 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2025-09-19 12:52:41,675 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2025-09-19 12:52:41,675 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2025-09-19 12:52:41,675 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2025-09-19 12:52:41,676 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2025-09-19 12:52:41,683 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2025-09-19 12:52:41,685 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2025-09-19 12:52:41,691 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2025-09-19 12:52:41,692 INFO util.GSet: VM type       = 64-bit\n",
            "2025-09-19 12:52:41,692 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n",
            "2025-09-19 12:52:41,692 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2025-09-19 12:52:41,711 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2025-09-19 12:52:41,711 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2025-09-19 12:52:41,711 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2025-09-19 12:52:41,721 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2025-09-19 12:52:41,721 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2025-09-19 12:52:41,726 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2025-09-19 12:52:41,726 INFO util.GSet: VM type       = 64-bit\n",
            "2025-09-19 12:52:41,727 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 997.2 KB\n",
            "2025-09-19 12:52:41,728 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2025-09-19 12:52:41,784 INFO namenode.FSImage: Allocated new BlockPoolId: BP-849393588-172.28.0.12-1758286361773\n",
            "2025-09-19 12:52:41,825 INFO common.Storage: Storage directory /content/hdfs/namenode has been successfully formatted.\n",
            "2025-09-19 12:52:41,873 INFO namenode.FSImageFormatProtobuf: Saving image file /content/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2025-09-19 12:52:42,044 INFO namenode.FSImageFormatProtobuf: Image file /content/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2025-09-19 12:52:42,062 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2025-09-19 12:52:42,113 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2025-09-19 12:52:42,114 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2025-09-19 12:52:42,122 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2025-09-19 12:52:42,122 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 9c5f6c881f56/172.28.0.12\n",
            "************************************************************/\n",
            "Starting NameNode and DataNode…\n",
            "\n",
            "Java processes (via jps):\n",
            "11904 Jps\n",
            "11858 DataNode\n",
            "11796 NameNode\n",
            "\n",
            "Check HDFS root directory:\n",
            "Found 2 items\n",
            "drwxr-xr-x   - root supergroup          0 2025-09-19 12:52 /data\n",
            "drwxr-xr-x   - root supergroup          0 2025-09-19 12:52 /user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Start HDFS and prep dirs"
      ],
      "metadata": {
        "id": "9VVLg8mM8rCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_HOME = \"/usr/local/hadoop\"\n",
        "\n",
        "# Start daemons\n",
        "!$HADOOP_HOME/bin/hdfs --daemon start namenode\n",
        "!$HADOOP_HOME/bin/hdfs --daemon start datanode\n",
        "\n",
        "# Show Java processes\n",
        "!jps || true\n",
        "\n",
        "# Prep common HDFS dirs\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root || true\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir -p /data || true"
      ],
      "metadata": {
        "id": "yYKaBqvW8zf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7970dd3-31b8-43be-fd53-f417fe9eb4c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "namenode is running as process 11796.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n",
            "datanode is running as process 11858.  Stop it first and ensure /tmp/hadoop-root-datanode.pid file is empty before retry.\n",
            "12528 Jps\n",
            "11858 DataNode\n",
            "11796 NameNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Install KaggleHub & load dataset"
      ],
      "metadata": {
        "id": "w5YuTMb085eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install kagglehub[pandas-datasets]\n",
        "\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Dataset slug and file name (adjust the file name if needed)\n",
        "DATASET = \"maharshipandya/-spotify-tracks-dataset\"\n",
        "DATAFILE = \"dataset.csv\"\n",
        "\n",
        "# Load directly to pandas\n",
        "df = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    DATASET,\n",
        "    DATAFILE\n",
        ")\n",
        "\n",
        "# Save locally for HDFS upload\n",
        "LOCAL_CSV = \"/content/spotify.csv\"\n",
        "df.to_csv(LOCAL_CSV, index=False)"
      ],
      "metadata": {
        "id": "HqyF6uW-88KE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b094e050-cd87-4946-91a4-bd92c8d82c8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2206540325.py:11: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the '-spotify-tracks-dataset' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Upload to HDFS"
      ],
      "metadata": {
        "id": "5zWwCqPZ9MCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_HOME = \"/usr/local/hadoop\"\n",
        "HDFS_TARGET_DIR = \"/data/spotify\"\n",
        "\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir -p $HDFS_TARGET_DIR\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put -f /content/spotify.csv $HDFS_TARGET_DIR\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls -h $HDFS_TARGET_DIR"
      ],
      "metadata": {
        "id": "ZK_Libnk9OMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a156930c-ebb7-4e1f-8e17-831e231f0ef9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     19.2 M 2025-09-19 12:59 /data/spotify/spotify.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Verify HDFS upload + PySpark sanity check"
      ],
      "metadata": {
        "id": "FFxaoByQ9P3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the file in HDFS\n",
        "HADOOP_HOME = \"/usr/local/hadoop\"\n",
        "HDFS_PATH = \"hdfs://localhost:9000/data/spotify/spotify.csv\"\n",
        "\n",
        "print(\"Listing HDFS target directory:\")\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls -h /data/spotify || true\n",
        "\n",
        "print(\"\\nFirst few lines of the CSV in HDFS:\")\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /data/spotify/spotify.csv | head -n 5 || true\n",
        "\n",
        "# Install PySpark\n",
        "!pip -q install pyspark\n",
        "\n",
        "# Start Spark and point it at HDFS\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F, types as T\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SpotifyHDFSCheck\") \\\n",
        "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the CSV from HDFS\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(HDFS_PATH)\n",
        "\n",
        "print(\"\\n=== DataFrame Schema ===\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\n=== Row Count ===\")\n",
        "print(df.count())\n",
        "\n",
        "print(\"\\n=== Sample Rows ===\")\n",
        "df.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "ADc3L5Ac9UhI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47e4300-4d98-4ce3-a8a8-4fee4a565345"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing HDFS target directory:\n",
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     19.2 M 2025-09-19 12:59 /data/spotify/spotify.csv\n",
            "\n",
            "First few lines of the CSV in HDFS:\n",
            "Unnamed: 0,track_id,artists,album_name,track_name,popularity,duration_ms,explicit,danceability,energy,key,loudness,mode,speechiness,acousticness,instrumentalness,liveness,valence,tempo,time_signature,track_genre\n",
            "0,5SuOikwiRyPMVoIQDJUgSV,Gen Hoshino,Comedy,Comedy,73,230666,False,0.676,0.461,1,-6.746,0,0.143,0.0322,1.01e-06,0.358,0.715,87.917,4,acoustic\n",
            "1,4qPNDBW1i3p13qLCt0Ki3A,Ben Woodward,Ghost (Acoustic),Ghost - Acoustic,55,149610,False,0.42,0.166,1,-17.235,1,0.0763,0.924,5.56e-06,0.101,0.267,77.489,4,acoustic\n",
            "2,1iJBSr7s7jYXzM8EGcbK5b,Ingrid Michaelson;ZAYN,To Begin Again,To Begin Again,57,210826,False,0.438,0.359,0,-9.734,1,0.0557,0.21,0.0,0.117,0.12,76.332,4,acoustic\n",
            "3,6lfxq3CG4xtTiEg7opyCyx,Kina Grannis,Crazy Rich Asians (Original Motion Picture Soundtrack),Can't Help Falling In Love,71,201933,False,0.266,0.0596,0,-18.515,1,0.0363,0.905,7.07e-05,0.132,0.143,181.74,3,acoustic\n",
            "cat: Unable to write to output stream.\n",
            "\n",
            "=== DataFrame Schema ===\n",
            "root\n",
            " |-- Unnamed: 0: integer (nullable = true)\n",
            " |-- track_id: string (nullable = true)\n",
            " |-- artists: string (nullable = true)\n",
            " |-- album_name: string (nullable = true)\n",
            " |-- track_name: string (nullable = true)\n",
            " |-- popularity: string (nullable = true)\n",
            " |-- duration_ms: string (nullable = true)\n",
            " |-- explicit: string (nullable = true)\n",
            " |-- danceability: string (nullable = true)\n",
            " |-- energy: string (nullable = true)\n",
            " |-- key: string (nullable = true)\n",
            " |-- loudness: string (nullable = true)\n",
            " |-- mode: string (nullable = true)\n",
            " |-- speechiness: string (nullable = true)\n",
            " |-- acousticness: string (nullable = true)\n",
            " |-- instrumentalness: double (nullable = true)\n",
            " |-- liveness: string (nullable = true)\n",
            " |-- valence: string (nullable = true)\n",
            " |-- tempo: double (nullable = true)\n",
            " |-- time_signature: double (nullable = true)\n",
            " |-- track_genre: string (nullable = true)\n",
            "\n",
            "\n",
            "=== Row Count ===\n",
            "114000\n",
            "\n",
            "=== Sample Rows ===\n",
            "+----------+----------------------+----------------------+------------------------------------------------------+--------------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+\n",
            "|Unnamed: 0|track_id              |artists               |album_name                                            |track_name                |popularity|duration_ms|explicit|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|tempo  |time_signature|track_genre|\n",
            "+----------+----------------------+----------------------+------------------------------------------------------+--------------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+\n",
            "|0         |5SuOikwiRyPMVoIQDJUgSV|Gen Hoshino           |Comedy                                                |Comedy                    |73        |230666     |False   |0.676       |0.461 |1  |-6.746  |0   |0.143      |0.0322      |1.01E-6         |0.358   |0.715  |87.917 |4.0           |acoustic   |\n",
            "|1         |4qPNDBW1i3p13qLCt0Ki3A|Ben Woodward          |Ghost (Acoustic)                                      |Ghost - Acoustic          |55        |149610     |False   |0.42        |0.166 |1  |-17.235 |1   |0.0763     |0.924       |5.56E-6         |0.101   |0.267  |77.489 |4.0           |acoustic   |\n",
            "|2         |1iJBSr7s7jYXzM8EGcbK5b|Ingrid Michaelson;ZAYN|To Begin Again                                        |To Begin Again            |57        |210826     |False   |0.438       |0.359 |0  |-9.734  |1   |0.0557     |0.21        |0.0             |0.117   |0.12   |76.332 |4.0           |acoustic   |\n",
            "|3         |6lfxq3CG4xtTiEg7opyCyx|Kina Grannis          |Crazy Rich Asians (Original Motion Picture Soundtrack)|Can't Help Falling In Love|71        |201933     |False   |0.266       |0.0596|0  |-18.515 |1   |0.0363     |0.905       |7.07E-5         |0.132   |0.143  |181.74 |3.0           |acoustic   |\n",
            "|4         |5vjLSffimiIP26QG5WcN2K|Chord Overstreet      |Hold On                                               |Hold On                   |82        |198853     |False   |0.618       |0.443 |2  |-9.681  |1   |0.0526     |0.469       |0.0             |0.0829  |0.167  |119.949|4.0           |acoustic   |\n",
            "+----------+----------------------+----------------------+------------------------------------------------------+--------------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Write a persistent Parquet copy of the dataset to Drive"
      ],
      "metadata": {
        "id": "tjtayEN5KuTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PersistToDrive\") \\\n",
        "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sdf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"hdfs://localhost:9000/data/spotify/spotify.csv\")\n",
        "\n",
        "# Write Parquet to Drive (persistent)\n",
        "BRONZE = \"file:///content/drive/MyDrive/data/spotify/bronze_parquet\"\n",
        "sdf.write.mode(\"overwrite\").parquet(BRONZE)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j8yfAZgK44z",
        "outputId": "0543ad44-0c57-43b1-b570-aa4e986be65a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}
